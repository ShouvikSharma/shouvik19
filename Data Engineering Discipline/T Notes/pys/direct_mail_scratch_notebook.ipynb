{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9ba232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1721615106335_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-240-119-181.us-east-2.compute.internal:20888/proxy/application_1721615106335_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-240-118-129.us-east-2.compute.internal:8042/node/containerlogs/container_1721615106335_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741aebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264fe1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd32d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50be71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c9c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b3737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2f05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aceac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3895f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d349294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/paramiko/pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/usr/local/lib/python3.7/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "/usr/local/lib/python3.7/site-packages/paramiko/transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "import zipfile\n",
    "import pytz\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize \n",
    "import json\n",
    "import gnupg\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import paramiko\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "import typing\n",
    "from os.path import join, dirname\n",
    "import time\n",
    "import logging\n",
    "from paramiko import SFTPClient, SFTPFile, Message, SFTPError, Transport\n",
    "from paramiko.sftp import CMD_STATUS, CMD_READ, CMD_DATA\n",
    "\n",
    "logger = logging.getLogger('demo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed4fea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class _SFTPFileDownloader:\n",
    "    \"\"\"\n",
    "    Helper class to download large file with paramiko sftp client with limited number of concurrent requests.\n",
    "    \"\"\"\n",
    "\n",
    "    _DOWNLOAD_MAX_REQUESTS = 48\n",
    "    _DOWNLOAD_MAX_CHUNK_SIZE = 0x8000\n",
    "\n",
    "    def __init__(self, f_in: SFTPFile, f_out: typing.BinaryIO, callback=None):\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        self.callback = callback\n",
    "\n",
    "        self.requested_chunks = {}\n",
    "        self.received_chunks = {}\n",
    "        self.saved_exception = None\n",
    "\n",
    "    def download(self):\n",
    "        file_size = self.f_in.stat().st_size\n",
    "        requested_size = 0\n",
    "        received_size = 0\n",
    "\n",
    "        while True:\n",
    "            # send read requests\n",
    "            while len(self.requested_chunks) + len(self.received_chunks) < self._DOWNLOAD_MAX_REQUESTS and \\\n",
    "                    requested_size < file_size:\n",
    "                chunk_size = min(self._DOWNLOAD_MAX_CHUNK_SIZE, file_size - requested_size)\n",
    "                request_id = self._sftp_async_read_request(\n",
    "                    fileobj=self,\n",
    "                    file_handle=self.f_in.handle,\n",
    "                    offset=requested_size,\n",
    "                    size=chunk_size\n",
    "                )\n",
    "                self.requested_chunks[request_id] = (requested_size, chunk_size)\n",
    "                requested_size += chunk_size\n",
    "\n",
    "            # receive blocks if they are available\n",
    "            # note: the _async_response is invoked\n",
    "            self.f_in.sftp._read_response()\n",
    "            self._check_exception()\n",
    "\n",
    "            # write received data to output stream\n",
    "            while True:\n",
    "                chunk = self.received_chunks.pop(received_size, None)\n",
    "                if chunk is None:\n",
    "                    break\n",
    "                _, chunk_size, chunk_data = chunk\n",
    "                self.f_out.write(chunk_data)\n",
    "                if self.callback is not None:\n",
    "                    self.callback(chunk_data)\n",
    "\n",
    "                received_size += chunk_size\n",
    "\n",
    "            # check transfer status\n",
    "            if received_size >= file_size:\n",
    "                break\n",
    "\n",
    "            # check chunks queues\n",
    "            if not self.requested_chunks and len(self.received_chunks) >= self._DOWNLOAD_MAX_REQUESTS:\n",
    "                raise ValueError(\"SFTP communication error. The queue with requested file chunks is empty and\"\n",
    "                                 \"the received chunks queue is full and cannot be consumed.\")\n",
    "\n",
    "        return received_size\n",
    "\n",
    "    def _sftp_async_read_request(self, fileobj, file_handle, offset, size):\n",
    "        sftp_client = self.f_in.sftp\n",
    "\n",
    "        with sftp_client._lock:\n",
    "            num = sftp_client.request_number\n",
    "\n",
    "            msg = Message()\n",
    "            msg.add_int(num)\n",
    "            msg.add_string(file_handle)\n",
    "            msg.add_int64(offset)\n",
    "            msg.add_int(size)\n",
    "\n",
    "            sftp_client._expecting[num] = fileobj\n",
    "            sftp_client.request_number += 1\n",
    "\n",
    "        sftp_client._send_packet(CMD_READ, msg)\n",
    "        return num\n",
    "\n",
    "    def _async_response(self, t, msg, num):\n",
    "        if t == CMD_STATUS:\n",
    "            # save exception and re-raise it on next file operation\n",
    "            try:\n",
    "                self.f_in.sftp._convert_status(msg)\n",
    "            except Exception as e:\n",
    "                self.saved_exception = e\n",
    "            return\n",
    "        if t != CMD_DATA:\n",
    "            raise SFTPError(\"Expected data\")\n",
    "        data = msg.get_string()\n",
    "\n",
    "        chunk_data = self.requested_chunks.pop(num, None)\n",
    "        if chunk_data is None:\n",
    "            return\n",
    "\n",
    "        # save chunk\n",
    "        offset, size = chunk_data\n",
    "\n",
    "        if size != len(data):\n",
    "            raise SFTPError(f\"Invalid data block size. Expected {size} bytes, but it has {len(data)} size\")\n",
    "        self.received_chunks[offset] = (offset, size, data)\n",
    "\n",
    "    def _check_exception(self):\n",
    "        \"\"\"if there's a saved exception, raise & clear it\"\"\"\n",
    "        if self.saved_exception is not None:\n",
    "            x = self.saved_exception\n",
    "            self.saved_exception = None\n",
    "            raise x\n",
    "\n",
    "\n",
    "def download_file(sftp_client: SFTPClient, remote_path: str, local_path: str, callback=None):\n",
    "    \"\"\"\n",
    "    Helper function to download remote file via sftp.\n",
    "    It contains a fix for a bug that prevents a large file downloading with :meth:`paramiko.SFTPClient.get`\n",
    "    Note: this function relies on some private paramiko API and has been tested with paramiko 2.7.1.\n",
    "          So it may not work with other paramiko versions.\n",
    "    :param sftp_client: paramiko sftp client\n",
    "    :param remote_path: remote file path\n",
    "    :param local_path: local file path\n",
    "    :param callback: optional data callback\n",
    "    \"\"\"\n",
    "    remote_file_size = sftp_client.stat(remote_path).st_size\n",
    "\n",
    "    with sftp_client.open(remote_path, 'rb') as f_in, open(local_path, 'wb') as f_out:\n",
    "        _SFTPFileDownloader(\n",
    "            f_in=f_in,\n",
    "            f_out=f_out,\n",
    "            callback=callback\n",
    "        ).download()\n",
    "\n",
    "    local_file_size = os.path.getsize(local_path)\n",
    "    if remote_file_size != local_file_size:\n",
    "        raise IOError(f\"file size mismatch: {remote_file_size} != {local_file_size}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    start_time = time.time()\n",
    "    # public sftp server\n",
    "    host = 'DOTS.datalabusa.com'\n",
    "    port = 22\n",
    "    username = 'ex_Avant'\n",
    "    password = 'Et#h@ut8Ec2F'\n",
    "    remote_file_path = 'To_Datalab/AV_SUPP_FUNDED_07212024.csv.gpg'\n",
    "#     local_file_path = join(dirname(__file__), 'DL_AVT_INTERIMFILE2_202408_HI.csv.pgp')\n",
    "    local_file_path = '/tmp/AV_SUPP_FUNDED_07212024.csv.gpg'\n",
    "\n",
    "    transport = Transport((host, port))\n",
    "    transport.set_keepalive(30)\n",
    "    transport.connect(\n",
    "        username=username,\n",
    "        password=password,\n",
    "    )\n",
    "    \n",
    "    with SFTPClient.from_transport(transport) as sftp_client:\n",
    "        progress_size = 0\n",
    "        total_size = 0\n",
    "        step_size = 4 * 1024 * 1024\n",
    "\n",
    "        def progress_callback(data):\n",
    "            nonlocal progress_size, total_size\n",
    "            progress_size += len(data)\n",
    "            total_size += len(data)\n",
    "            while progress_size >= step_size:\n",
    "                logger.info(f\"{total_size // (1024 ** 2)} MB has been downloaded\")\n",
    "                progress_size -= step_size\n",
    "        \n",
    "        download_file(\n",
    "            sftp_client=sftp_client,\n",
    "            remote_path=remote_file_path,\n",
    "            local_path=local_file_path,\n",
    "            callback=progress_callback\n",
    "        )\n",
    "    download_time = time.time() - start_time\n",
    "    print(f\"File downloaded successfully to {local_file_path} in {download_time:.2f} seconds\")\n",
    "\n",
    "    # s4 upload\n",
    "    s3_bucket_name = 'avant-partner01-landing'\n",
    "    s3_object_key = 'datalabs/drop/interim/AV_SUPP_FUNDED_07212024.csv.gpg'\n",
    "\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Step 1: Initiate the multipart upload\n",
    "    response = s3_client.create_multipart_upload(Bucket=s3_bucket_name, Key=s3_object_key)\n",
    "    upload_id = response['UploadId']\n",
    "    print(f'Upload ID: {upload_id}')\n",
    "\n",
    "    # Step 2: Upload parts\n",
    "    part_size = 10 * 1024 * 1024  # 5 MB\n",
    "    part_info = {'Parts': []}\n",
    "    part_number = 1\n",
    "\n",
    "    with open(local_file_path, 'rb') as file:\n",
    "        while True:\n",
    "            data = file.read(part_size)\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            # Generate pre-signed URL for the part\n",
    "            presigned_url = s3_client.generate_presigned_url(\n",
    "                ClientMethod='upload_part',\n",
    "                Params={\n",
    "                    'Bucket': s3_bucket_name,\n",
    "                    'Key': s3_object_key,\n",
    "                    'UploadId': upload_id,\n",
    "                    'PartNumber': part_number,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Upload the part\n",
    "            response = requests.put(presigned_url, data=data)\n",
    "            print(response)\n",
    "            etag = response.headers['ETag']\n",
    "\n",
    "            # Keep track of uploaded part details\n",
    "            part_info['Parts'].append({\n",
    "                'PartNumber': part_number\n",
    "            })\n",
    "\n",
    "            print(f'Uploaded part {part_number} ')\n",
    "            part_number += 1\n",
    "\n",
    "    # Step 3: Complete the multipart upload\n",
    "    response = s3_client.complete_multipart_upload(\n",
    "        Bucket=s3_bucket_name,\n",
    "        Key=s3_object_key,\n",
    "        UploadId=upload_id,\n",
    "        MultipartUpload=part_info\n",
    "    )\n",
    "    print(f'Multipart upload completed: {response}')\n",
    "\n",
    "    # Clean up local file\n",
    "    os.remove(local_file_path)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s]: %(message)s\")\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8c9d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'etag'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 218, in main\n",
      "  File \"/usr/local/lib/python3.7/site-packages/requests/structures.py\", line 52, in __getitem__\n",
      "    return self._store[key.lower()][1]\n",
      "KeyError: 'etag'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0230de52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload ID: wPdE1WJlAVBbbbQvxgTwQ2HDJezOf9u4AZOlRL9o29cCzl9ap5Xeih1bY3Gr6WawAMxeRLSk_ANEaEn573H71g.1nvz7UL61BxjCw4KAkN729bJGo8gzpGGyS7votcz0STfmowpi7cBB_MFcFBbdFKfxi.m2hdk5bTD1HHrVHhc-"
     ]
    }
   ],
   "source": [
    "# s4 upload\n",
    "s3_bucket_name = 'avant-partner01-landing'\n",
    "s3_object_key = 'datalabs/drop/interim/AV_SUPP_FUNDED_07212024.csv.gpg'\n",
    "\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Step 1: Initiate the multipart upload\n",
    "response = s3_client.create_multipart_upload(Bucket=s3_bucket_name, Key=s3_object_key)\n",
    "upload_id = response['UploadId']\n",
    "print(f'Upload ID: {upload_id}')\n",
    "\n",
    "# Step 2: Upload parts\n",
    "part_size = 10 * 1024 * 1024  # 5 MB\n",
    "part_info = {'Parts': []}\n",
    "part_number = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f76544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred (MalformedXML) when calling the CompleteMultipartUpload operation: The XML you provided was not well-formed or did not validate against our published schema\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/botocore/client.py\", line 508, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/botocore/client.py\", line 915, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.exceptions.ClientError: An error occurred (MalformedXML) when calling the CompleteMultipartUpload operation: The XML you provided was not well-formed or did not validate against our published schema\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/tmp/AV_SUPP_FUNDED_07212024.csv.gpg', 'rb') as file:\n",
    "    while True:\n",
    "        data = file.read(part_size)\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # Generate pre-signed URL for the part\n",
    "        presigned_url = s3_client.generate_presigned_url(\n",
    "            ClientMethod='upload_part',\n",
    "            Params={\n",
    "                'Bucket': s3_bucket_name,\n",
    "                'Key': s3_object_key,\n",
    "                'UploadId': upload_id,\n",
    "                'PartNumber': part_number,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        # Upload the part\n",
    "        response = requests.put(presigned_url, data=data)\n",
    "        print(response)\n",
    "#         etag = response.headers['ETag']\n",
    "\n",
    "        # Keep track of uploaded part details\n",
    "        part_info['Parts'].append({\n",
    "            'PartNumber': part_number\n",
    "        })\n",
    "\n",
    "        print(f'Uploaded part {part_number} ')\n",
    "        part_number += 1\n",
    "\n",
    "# Step 3: Complete the multipart upload\n",
    "response = s3_client.complete_multipart_upload(\n",
    "    Bucket=s3_bucket_name,\n",
    "    Key=s3_object_key,\n",
    "    UploadId=upload_id,\n",
    "    MultipartUpload=part_info\n",
    ")\n",
    "print(f'Multipart upload completed: {response}')\n",
    "\n",
    "# Clean up local file\n",
    "os.remove(local_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e572a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'local_file_path' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'local_file_path' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    with open(local_file_path, 'rb') as file:\n",
    "        while True:\n",
    "            data = file.read(part_size)\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            # Generate pre-signed URL for the part\n",
    "            presigned_url = s3_client.generate_presigned_url(\n",
    "                ClientMethod='upload_part',\n",
    "                Params={\n",
    "                    'Bucket': s3_bucket_name,\n",
    "                    'Key': s3_object_key,\n",
    "                    'UploadId': upload_id,\n",
    "                    'PartNumber': part_number,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Upload the part\n",
    "            response = requests.put(presigned_url, data=data)\n",
    "            print(response)\n",
    "            etag = response.headers['ETag']\n",
    "\n",
    "            # Keep track of uploaded part details\n",
    "            part_info['Parts'].append({\n",
    "                'PartNumber': part_number,\n",
    "                'ETag': etag\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f419d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
