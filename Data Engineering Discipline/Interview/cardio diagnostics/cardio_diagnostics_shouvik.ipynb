{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24443b9-b25c-4f20-8e35-35a1918a4816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.services.global.avant.com/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[K     |████████████████████████████    | 277.3 MB 159.3 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 317.0 MB 56 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 100.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488511 sha256=dcfddd85c898e90e7a371184788ef457d8428b3d8b89c959c45b52e8585aa3ca\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/92/09/11/aa01d01a7f005fda8a66ad71d2be7f8aa341bddafb27eee3c7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05a4bd2-b5b3-4895-b572-eaa72ed08ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2720225-e774-4e67-b81f-8f0fa7fba394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/27 16:26:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"genome_data_read\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcbb97a-7010-4c23-8f54-e83f3b6c540c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-ssharm23:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>genome_data_read</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f77441a9880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2279be65-8e0d-422c-b26c-4aa3b14f5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sample_genome_data.csv',header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "651b4317-0aaf-4ebd-80be-d8a4b8acbf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+-----------+--------------------+--------+-----------+\n",
      "|GeneID|Chromosome|StartPosition|EndPosition|            Sequence|GeneName|VariantType|\n",
      "+------+----------+-------------+-----------+--------------------+--------+-----------+\n",
      "|     1|         4|        38398|      38498|AGTTCAGGTCTAGTGAT...| GName_1|        SNP|\n",
      "|     2|        11|        49330|      49430|AGCTCGGCAAGACGAGT...| GName_2|  Insertion|\n",
      "|     3|        17|        49355|      49455|GGTTGGTCACGGTCGAA...| GName_3|  Insertion|\n",
      "|     4|        15|        49699|      49799|ATCCGTCGTCGTCGGTC...| GName_4|        SNP|\n",
      "|     5|        19|        34128|      34228|GTATTGGGCTCACACGA...| GName_5|  Insertion|\n",
      "+------+----------+-------------+-----------+--------------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00d75f-8803-49f6-91ea-bb3a02cbcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create table genome_date\n",
    "(\n",
    "    gene_id int increment() pk,\n",
    "    chromosome int,\n",
    "    start_position bigint,\n",
    "    end_position bigint,\n",
    "    genome_sequence string/varchar,\n",
    "    gene_name string,\n",
    "    variant_type string\n",
    ")\n",
    "USING 'S3:////'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972dfc8-ca7b-4793-93cc-c02776a40e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e2f17bf-4f88-4082-9168-da239c0eddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,length\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import IntegerType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce4a143a-f5f8-4fa1-8bdb-3706e85127ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ATCG'.count('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20e8ea69-518e-41fd-ad5c-ac0e80e0487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_function(string):\n",
    "    count_of_ATCG = string.count('ATCG')\n",
    "    return count_of_ATCG\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7989c7c6-214f-415e-b5e7-d3e4b44c4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_udf = udf(count_function,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cc80df2-0055-4009-b974-94b336a5622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenct_count = df.select([\"*\",pyspark_udf(\"Sequence\").alias(\"sequence_count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdd9f72a-7639-47a5-9174-e1fb1098ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+-----------+--------------------+--------+-----------+\n",
      "|GeneID|Chromosome|StartPosition|EndPosition|            Sequence|GeneName|VariantType|\n",
      "+------+----------+-------------+-----------+--------------------+--------+-----------+\n",
      "|     1|         4|        38398|      38498|AGTTCAGGTCTAGTGAT...| GName_1|        SNP|\n",
      "|     2|        11|        49330|      49430|AGCTCGGCAAGACGAGT...| GName_2|  Insertion|\n",
      "|     3|        17|        49355|      49455|GGTTGGTCACGGTCGAA...| GName_3|  Insertion|\n",
      "|     4|        15|        49699|      49799|ATCCGTCGTCGTCGGTC...| GName_4|        SNP|\n",
      "|     5|        19|        34128|      34228|GTATTGGGCTCACACGA...| GName_5|  Insertion|\n",
      "+------+----------+-------------+-----------+--------------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "635d46cf-8ae8-4424-9563-4464e4267393",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_212/1412151369.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequenct_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3125\u001b[0m         \"\"\"\n\u001b[1;32m   3126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3127\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3128\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3129\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "final_df = df.concat(sequenct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3a819c1-4254-4312-ade2-32cf65cdd643",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "withColumn() missing 1 required positional argument: 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_212/3415799511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sequence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sequence_count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: withColumn() missing 1 required positional argument: 'col'"
     ]
    }
   ],
   "source": [
    "df.withColumn(col(\"Sequence\")).rdd.map(count_function).alias(col('sequence_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c7743-d496-4ba9-a06a-b5db00f08873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
